{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKfDEAsPyfVm"
   },
   "source": [
    "Can run in Colab\n",
    "Version: Jan 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfPM8crA8HEG",
    "origin_pos": 0
   },
   "source": [
    "# Concise Implementation of Linear Regression\n",
    "\n",
    "## Generating the Dataset\n",
    "\n",
    "To start, we will generate the same dataset as in :numref:`sec_linear_scratch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2bqIv9eS8HEH",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ctw-Jza5yIgn"
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = Xw + b + noise.\n",
    "        Return X and y\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b  #torch.mv does not broadcast so use matmul\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RhenXasv8HEH",
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpObKDPx8HEH",
    "origin_pos": 5
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Rather than rolling our own iterator,\n",
    "we can call upon the existing API in a framework to read data.\n",
    "We pass in `features` and `labels` as arguments and specify `batch_size`\n",
    "when instantiating a data iterator object.\n",
    "Besides, the boolean value `is_train`\n",
    "indicates whether or not\n",
    "we want the data iterator object to shuffle the data\n",
    "on each epoch (pass through the dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0oSDO3l48HEI",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FRmAwH1O8HEI",
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n",
      "[tensor([[-0.0039, -1.5041],\n",
      "        [ 1.3929,  0.2728],\n",
      "        [ 0.1892, -0.0184],\n",
      "        [-1.0933,  0.5864],\n",
      "        [ 1.2372,  0.6163],\n",
      "        [-1.0500,  0.3037],\n",
      "        [-0.2264, -0.8773],\n",
      "        [-0.0738, -0.1811],\n",
      "        [-1.8291, -0.3908],\n",
      "        [-0.7532, -0.5771]]), tensor([[9.2941],\n",
      "        [6.0587],\n",
      "        [4.6453],\n",
      "        [0.0245],\n",
      "        [4.5793],\n",
      "        [1.0715],\n",
      "        [6.7165],\n",
      "        [4.6761],\n",
      "        [1.8675],\n",
      "        [4.6562]])]\n"
     ]
    }
   ],
   "source": [
    "print(type(data_iter))\n",
    "print(type(iter(data_iter)))\n",
    "print(next(iter(data_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28miter\u001b[39m(test_list)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "test_list = [5,3,7,0]\n",
    "iter(test_list)\n",
    "\n",
    "iter(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4ztTXZl8HEI",
    "origin_pos": 10
   },
   "source": [
    "Now we can use `data_iter` in much the same way as we called\n",
    "the `data_iter` function in :numref:`sec_linear_scratch`.\n",
    "To verify that it is working, we can read and print\n",
    "the first minibatch of examples.\n",
    "Comparing with :numref:`sec_linear_scratch`,\n",
    "here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tknTF8q8HEI",
    "origin_pos": 12
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "For standard operations, we can use a framework's predefined layers,\n",
    "which allow us to focus especially\n",
    "on the layers used to construct the model\n",
    "rather than having to focus on the implementation.\n",
    "\n",
    "### nn.Sequential\n",
    "We will first define a model variable `net`,\n",
    "which will refer to an instance of the [`Sequential` class](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "\n",
    "**The `Sequential` class defines an ordered container\n",
    "of modules.** The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network with several layers that will be chained together\n",
    "\n",
    "Given input data, a `Sequential` instance passes it through\n",
    "the first layer, in turn passing the output\n",
    "as the second layer's input and so forth.\n",
    "In the following example, our model consists of only one layer,\n",
    "so we do not really need `Sequential`.\n",
    "But since nearly all of our future models\n",
    "will involve multiple layers,\n",
    "we will use it anyway just to familiarize you\n",
    "with the most standard workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8Mzaki58HEJ",
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "\n",
    "### nn.Linear\n",
    "In PyTorch, the fully-connected layer is defined in the [`Linear` class](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). This module applies a linear transformation on the input using its stored weights and biases.\n",
    "\n",
    "We pass two arguments into `nn.Linear`. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IWNsi2048HEJ",
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# `nn` is an abbreviation for neural networks\n",
    "from torch import nn\n",
    "\n",
    "# linear regression with 2 features\n",
    "#Define and initialise model\n",
    "net = nn.Sequential(nn.Linear(in_features=2, out_features=1))\n",
    "\n",
    "# nn.ReLU()\n",
    "\n",
    "print(type(net))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SV7DRsW8HEJ",
    "origin_pos": 19
   },
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "Before using `net`, we need to initialize the model parameters.\n",
    "\n",
    "Here we specify that\n",
    "\n",
    "-  each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01.\n",
    "-  The bias parameter will be initialized to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kedPSx08HEJ",
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "As we have specified the input and output dimensions when constructing `nn.Linear`,\n",
    "now we can access the parameters directly to specify their initial values.\n",
    "We first locate the layer by `net[0]`, which is the first layer in the network,\n",
    "and then use the `weight.data` and `bias.data` methods to access the parameters.\n",
    "Next we use the replace methods `normal_` and `fill_` to overwrite parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "net[0]\n",
    "print(type(net[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706178028611,
     "user": {
      "displayName": "Phil Chan",
      "userId": "01447778780384098326"
     },
     "user_tz": 0
    },
    "id": "YkVrWTP88HEJ",
    "origin_pos": 24,
    "outputId": "b2e7bf53-37af-40a1-d876-1c79cb0f1f96",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0100, -0.0092]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#Initialise weights\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "\n",
    "#Initialise bias\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "#print out initialised weight and bias\n",
    "print(net[0].weight.data)\n",
    "print(net[0].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtekkeV68HEK",
    "origin_pos": 29
   },
   "source": [
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUPo9Mdd8HEK",
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).\n",
    "By default it returns the average loss over examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vzBamysC8HEL",
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#set loss to MSE\n",
    "# Initialise loss function\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF2LhKjW8HEL",
    "origin_pos": 36
   },
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_t8aVX98HEL",
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Minibatch stochastic gradient descent is a standard tool\n",
    "for optimizing neural networks\n",
    "and thus PyTorch supports it alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we instantiate an `SGD` instance,\n",
    "we will specify the parameters to optimize over\n",
    "(obtainable from our net via `net.parameters()`), with a dictionary of hyperparameters\n",
    "required by our optimization algorithm.\n",
    "Minibatch stochastic gradient descent just requires that\n",
    "we set the value `lr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "iQ7aCJnz8HEL",
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x117c31dd0>\n",
      "<bound method Module.parameters of Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0100, -0.0092]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose SGD with learning rate, say, 0.03\n",
    "#Initialise SGD\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "print(net.parameters())\n",
    "print(net.parameters)\n",
    "test_net_param = net.parameters()\n",
    "next(test_net_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLPqzlqA8HEL",
    "origin_pos": 43
   },
   "source": [
    "## Training\n",
    "\n",
    "The training loop itself is strikingly similar\n",
    "to what we did when implementing everything from scratch.\n",
    "\n",
    "To refresh your memory: for some number of epochs,\n",
    "we will make a complete pass over the dataset (`train_data`),\n",
    "iteratively grabbing one minibatch of inputs\n",
    "and the corresponding ground-truth labels.\n",
    "For each minibatch, we go through the following ritual:\n",
    "\n",
    "* Generate predictions by calling `net(X)` and calculate the loss `l` (the forward pass/propagation).\n",
    "* Calculate gradients by running the backpropagation.\n",
    "* Update the model parameters by invoking our optimizer.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1706178035492,
     "user": {
      "displayName": "Phil Chan",
      "userId": "01447778780384098326"
     },
     "user_tz": 0
    },
    "id": "BMF-YY7_8HEL",
    "origin_pos": 45,
    "outputId": "03904966-2bf0-4347-e5d1-09ac0b98f041",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000261\n",
      "epoch 2, loss 0.000100\n",
      "epoch 3, loss 0.000101\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        #===forward pass============\n",
    "        l = loss(net(X) ,y)\n",
    "\n",
    "        #===backward pass==============\n",
    "        trainer.zero_grad()    #reset gradient before backprop\n",
    "        l.backward()           #computed gradients using backprop\n",
    "\n",
    "        #=update params================\n",
    "        trainer.step()\n",
    "    #===log results after each epoch. Loss computed using all training set============\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8nTuTRb8HEM",
    "origin_pos": 47
   },
   "source": [
    "Below, we compare the model parameters learned by training on finite data\n",
    "and the actual parameters that generated our dataset.\n",
    "To access parameters,\n",
    "we first access the layer that we need from `net`\n",
    "and then access that layer's weights and bias.\n",
    "As in our from-scratch implementation,\n",
    "note that our estimated parameters are\n",
    "close to their ground-truth counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706178035492,
     "user": {
      "displayName": "Phil Chan",
      "userId": "01447778780384098326"
     },
     "user_tz": 0
    },
    "id": "MvueVCm88HEM",
    "origin_pos": 49,
    "outputId": "69edbacc-23d0-4321-bcc0-8350189e9e04",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([ 1.9900, -3.3908])\n",
      "error in estimating b: tensor([4.2000])\n"
     ]
    }
   ],
   "source": [
    "#Compare true to esimtated parameters\n",
    "w = net[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0100, -0.0092]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-E0pEpI8HEM",
    "origin_pos": 51
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhqb22Xb8HEM",
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "* Using PyTorch's high-level APIs, we can implement models much more concisely.\n",
    "* In PyTorch, the `data` module provides tools for data processing, the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "* We can initialize the parameters by replacing their values with methods ending with `_`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk5NUoRP8HEM",
    "origin_pos": 55
   },
   "source": [
    "## Activity\n",
    "Things to try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2YGhqce8HEM",
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. In SGD can we start with backpass? Try it.\n",
    "1. In training, split the single line of code that estimates the model and obtains the loss into 2 separate lines of code: 1 to give output, and one to compute the loss based on that output.\n",
    "1. If we replace `nn.MSELoss(reduction='sum')` with `nn.MSELoss()`, how can we change the learning rate for the code to behave identically. Why?\n",
    "1. Review the PyTorch documentation to see what loss functions and initialization methods are provided. Replace the loss by some other loss like [absolute(L1)](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) or [Huber loss](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html).\n",
    "\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/45)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1druzewkfc4iDEsO1p17zKPrafwBb_oWe",
     "timestamp": 1706428614166
    }
   ]
  },
  "kernelspec": {
   "display_name": "env_ST311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
