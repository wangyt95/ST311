{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVJQvfFfg3nx"
   },
   "source": [
    "# **Conv layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1709225934575,
     "user": {
      "displayName": "Phil Chan",
      "userId": "01447778780384098326"
     },
     "user_tz": 0
    },
    "id": "OLDs66w6zcVV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpckxBdbzp2l"
   },
   "source": [
    "In PyTorch, we can create a convolutional layer using nn.Conv2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1709225987268,
     "user": {
      "displayName": "Phil Chan",
      "userId": "01447778780384098326"
     },
     "user_tz": 0
    },
    "id": "BO_0sQpIf4Z4"
   },
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=3,  # number of input channels\n",
    "                 out_channels=7, # number of output channels\n",
    "                 kernel_size=5)  # size of the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptvp4Bdgf8PY"
   },
   "source": [
    "The conv layer expects as input a tensor in the format \"NCHW\", meaning that the dimensions of the tensor should follow the order:\n",
    "\n",
    "* batch size\n",
    "* channel\n",
    "* height\n",
    "* width\n",
    "\n",
    "For example, we can emulate a batch of 32 colour images, each of size 128x128, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-hTZ-_00f9to"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 124, 124])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 3, 128, 128)\n",
    "y = conv(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmtZs2HagA9a"
   },
   "source": [
    "The output tensor is also in the \"NCHW\" format. We still have 32 images, and 7 channels (consistent with out_channels of conv), and of size 124x124. If we added the appropriate padding to conv, namely padding = kernel_size // 2, then our output width and height should be consistent with the input width and height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1brL3fmigB0h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 128, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=7,\n",
    "                  kernel_size=5,\n",
    "                  padding=2)\n",
    "\n",
    "x = torch.randn(32, 3, 128, 128)\n",
    "y = conv2(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuZNhAh_gRei"
   },
   "source": [
    "# **Parameters of a Convolutional Layer**\n",
    "\n",
    "Recall that the trainable parameters of a fully-connected layer includes the network weights and biases. There is one weight for each connection, and one bias for each output unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ioRWN-iSgQcq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(fc_params) 2\n",
      "Weights: torch.Size([30, 100])\n",
      "Biases: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(100, 30)\n",
    "fc_params = list(fc.parameters())\n",
    "print(\"len(fc_params)\", len(fc_params))\n",
    "print(\"Weights:\", fc_params[0].shape)\n",
    "print(\"Biases:\", fc_params[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o2-ch5igcPA"
   },
   "source": [
    "In a convolutional layer, the trainable parameters include the convolutional kernels (filters) and also a set of biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e58__eEMgc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(conv_params): 2\n",
      "Filters: torch.Size([7, 3, 5, 5])\n",
      "Biases: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=7,\n",
    "                  kernel_size=5,\n",
    "                  padding=2)\n",
    "conv_params = list(conv2.parameters())\n",
    "print(\"len(conv_params):\", len(conv_params))\n",
    "print(\"Filters:\", conv_params[0].shape)\n",
    "print(\"Biases:\", conv_params[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCavI1UXghAZ"
   },
   "source": [
    "There is one bias for each output channel. Each bias is added to every element in that output channel. Note that the bias computation was not shown in the above figures, and are often omitted in other texts describing convolutional arithmetics. Nevertheless, the biases are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUjSUtTVgifA"
   },
   "source": [
    "# **Pooling layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "nENOZ2wTglS5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 128, 128]),\n",
       " torch.Size([32, 7, 128, 128]),\n",
       " torch.Size([32, 7, 64, 64]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2) # take the max over each 2-by-2 small blocks\n",
    "y = conv2(x)\n",
    "z = pool(y)\n",
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wljJpV1qgp3J"
   },
   "source": [
    "Usually, the kernel size and the stride length will be equal.\n",
    "\n",
    "The pooling layer has no trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_OwevkJEgqhg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pool.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-correlation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            # print(X[i:i+h, j:j+w])\n",
    "            # print(K)\n",
    "            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0],\n",
    "                  [3.0, 4.0, 5.0],\n",
    "                  [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0],\n",
    "                  [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.rand(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An experiment to show why kernel is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key take away from this experiment is that: each kernel can detect a particular kind of pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply gradient descent algorithm can allow the kernel $K$ to adjust its value so that $\\hat{Y} = conv2d(X,K)$ and $Y$ being close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss  1.667570\n",
      "epoch 4, loss  0.280130\n",
      "epoch 6, loss  0.047145\n",
      "epoch 8, loss  0.007970\n",
      "epoch 10, loss  0.001362\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1,2), bias=False)\n",
    "\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat-Y)**2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i+1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum(): 3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9926, -0.9940]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is close to $[[1, -1]]$, the theoritical solution that we have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Padding is to add `edges' for the original image, so that we can manipulate the shape of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1,1) + X.shape)\n",
    "    Y = conv2d(X)      # the default class nn.Conv2D takes a 4-dimensional tensor as its input\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3) # default padding=0\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(8,8))\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=(5,3), padding=(2,1)) # set different for vertical and horrisontal \n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up, down, left, right each direction add a zero, so 6 becomes 8 rather than 7.\n",
    "\n",
    "Let us then see a direct example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[19., 25.],\n",
      "          [37., 43.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Without padding\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(2,2), bias=False) \n",
    "conv2d.weight = nn.Parameter(torch.tensor([[[[0.0, 1.0],\n",
    "                                             [2.0, 3.0]]]]))\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0],\n",
    "                  [3.0, 4.0, 5.0],\n",
    "                  [6.0, 7.0, 8.0]])\n",
    "output = conv2d(X.reshape((1,1) + X.shape))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  3.,  8.,  4.],\n",
      "          [ 9., 19., 25., 10.],\n",
      "          [21., 37., 43., 16.],\n",
      "          [ 6.,  7.,  8.,  0.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Without padding\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(2,2), bias=False, padding=1) \n",
    "conv2d.weight = nn.Parameter(torch.tensor([[[[0.0, 1.0],\n",
    "                                             [2.0, 3.0]]]]))\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0],\n",
    "                  [3.0, 4.0, 5.0],\n",
    "                  [6.0, 7.0, 8.0]])\n",
    "output = conv2d(X.reshape((1,1) + X.shape))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "\n",
    "basically means skip some blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  8.],\n",
      "          [21., 43.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=2, padding=1, stride=2, bias=False)\n",
    "conv2d.weight = nn.Parameter(torch.tensor([[[[0.0, 1.0],\n",
    "                                             [2.0, 3.0]]]]))\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0],\n",
    "                  [3.0, 4.0, 5.0],\n",
    "                  [6.0, 7.0, 8.0]])\n",
    "output = conv2d(X.reshape((1,1) + X.shape))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoritically, the number of rows should be floor( (3 + 1*2 - 2 + 2)/2 ) = floor(2.5) = 2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1rIrrmxwI0TdFLIkR5hx6",
   "collapsed_sections": [
    "cVJQvfFfg3nx",
    "FuZNhAh_gRei",
    "VUjSUtTVgifA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_ST311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
